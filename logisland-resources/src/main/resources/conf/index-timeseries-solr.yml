version: 1.3.0
documentation: LogIsland future factory job

engine:
  component: com.hurence.logisland.engine.spark.KafkaStreamProcessingEngine
  configuration:
    spark.app.name: TimeseriesParsing
    spark.master: local[2]
    spark.streaming.batchDuration: 200
    spark.streaming.kafka.maxRatePerPartition: 10000
  controllerServiceConfigurations:

    - controllerService: datastore_service
      component: com.hurence.logisland.service.solr.Solr8ClientService
      configuration:
        solr.cloud: false
        solr.connection.string: http://localhost:8983/solr
        solr.collection: historian
        solr.concurrent.requests: 4
        flush.interval: 2000
        batch.size: 1000

    - controllerService: kafka_service
      component: com.hurence.logisland.stream.spark.structured.provider.KafkaStructuredStreamProviderService
      configuration:
        kafka.input.topics: logisland_raw
        kafka.output.topics: logisland_measures
        kafka.error.topics: logisland_errors
        kafka.input.topics.serializer: com.hurence.logisland.serializer.JsonSerializer
        kafka.output.topics.serializer: com.hurence.logisland.serializer.JsonSerializer
        kafka.error.topics.serializer: com.hurence.logisland.serializer.JsonSerializer
        kafka.metadata.broker.list: localhost:9092
        kafka.zookeeper.quorum: localhost:2181
        kafka.topic.autoCreate: true
        kafka.topic.default.partitions: 4
        kafka.topic.default.replicationFactor: 1

    - controllerService: kafka_service_out
      component: com.hurence.logisland.stream.spark.structured.provider.KafkaStructuredStreamProviderService
      configuration:
        kafka.input.topics: logisland_measures
        kafka.output.topics: logisland_metrics
        kafka.error.topics: logisland_errors
        kafka.input.topics.serializer: com.hurence.logisland.serializer.JsonSerializer
        kafka.output.topics.serializer: com.hurence.logisland.serializer.JsonSerializer
        kafka.error.topics.serializer: com.hurence.logisland.serializer.JsonSerializer
        kafka.metadata.broker.list: localhost:9092
        kafka.zookeeper.quorum: localhost:2181
        kafka.topic.autoCreate: true
        kafka.topic.default.partitions: 4
        kafka.topic.default.replicationFactor: 1

  streamConfigurations:

    # This stream take all raw events as lines comming from local files
    # these lines are split into logisland records and sent into a kafka topic
    - stream: parsing_stream
      component: com.hurence.logisland.stream.spark.structured.StructuredStream
      configuration:
        read.topics.serializer: none
        read.stream.service.provider: kafka_service
        write.topics.serializer: com.hurence.logisland.serializer.JsonSerializer
        write.stream.service.provider: kafka_service
      processorConfigurations:

        - processor: historian_parser
          component: com.hurence.logisland.processor.SplitText
          configuration:
            record.type: timeserie
            value.regex: (\S+\s+\S+);(\S+);(\S+);(\S+)
            value.fields: record_time,tagname,record_value,quality

        - processor: create_aliases
          component: com.hurence.logisland.processor.NormalizeFields
          configuration:
            conflict.resolution.policy: keep_both_fields
            record_name: tagname

        - processor: fields_types_converter
          component: com.hurence.logisland.processor.ConvertFieldsType
          configuration:
            record_value: double
            quality: float

    # This stream will perform a statefull groupBy operation on tagname
    - stream: compaction_stream
      component: com.hurence.logisland.stream.spark.structured.StructuredStream
      configuration:
        read.topics.key.serializer: com.hurence.logisland.serializer.StringSerializer
        read.topics.serializer: com.hurence.logisland.serializer.JsonSerializer
        read.stream.service.provider: kafka_service_out
        write.topics.serializer: com.hurence.logisland.serializer.JsonSerializer
        write.stream.service.provider: kafka_service_out
        groupby: tagname
        chunk.size: 100
        state.timeout.ms: 30000

      processorConfigurations:


        # Make one chronix chunk from all records
        - processor: timeseries_converter
          component: com.hurence.logisland.processor.ConvertToTimeseries
          configuration:
            groupby: tagname
            metric: avg;max;min;trend;sax:7,0.01,20

        - processor: create_aliases
          component: com.hurence.logisland.processor.NormalizeFields
          configuration:
            conflict.resolution.policy: keep_both_fields
            chunk_value: record_value

        # all the parsed records are added to solr by bulk
        - processor: solr_publisher
          component: com.hurence.logisland.processor.datastore.BulkPut
          configuration:
            datastore.client.service: datastore_service
