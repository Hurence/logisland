---  # document start

categories:
  -  processing
  -  parsing
  -  datastore
  -  alerting
  -  security
  -  enrichment
  -  analytics
  -  timeseries
extensions:
  - name: ComputeTags
    description:  >
      Compute tag cross from given formulas.
      
      - each dynamic property will return a new record according to the formula definition
      - the record name will be set to the property name
      - the record time will be set to the current timestamp
      
      a threshold_cross has the following properties : count, sum, avg, time, duration, value
    category: enrichment
    module: com.hurence.logisland:logisland-processor-common:1.1.2
    class: com.hurence.logisland.processor.alerting.ComputeTags
    tags: [record, fields, Add]
  - name: EnrichRecords
    description:  >
      Enrich input records with content indexed in datastore using multiget queries.
      Each incoming record must be possibly enriched with information stored in datastore. 
      The plugin properties are :
      
      - es.index (String)            : Name of the datastore index on which the multiget query will be performed. This field is mandatory and should not be empty, otherwise an error output record is sent for this specific incoming record.
      - record.key (String)          : Name of the field in the input record containing the id to lookup document in elastic search. This field is mandatory.
      - es.key (String)              : Name of the datastore key on which the multiget query will be performed. This field is mandatory.
      - includes (ArrayList<String>) : List of patterns to filter in (include) fields to retrieve. Supports wildcards. This field is not mandatory.
      - excludes (ArrayList<String>) : List of patterns to filter out (exclude) fields to retrieve. Supports wildcards. This field is not mandatory.
      
      Each outcoming record holds at least the input record plus potentially one or more fields coming from of one datastore document.
    category: enrichment
    module: com.hurence.logisland:logisland-processor-common:1.1.2
    class: com.hurence.logisland.processor.datastore.EnrichRecords
    tags: [datastore, enricher]
  - name: DebugStream
    description: This is a processor that logs incoming records
    category: utils
    module: com.hurence.logisland:logisland-processor-common:1.1.2
    class: com.hurence.logisland.processor.DebugStream
    tags: [record, debug]
  - name: BulkPut
    description: Indexes the content of a Record in a Datastore using bulk processor
    category: datastore
    module: com.hurence.logisland:logisland-processor-common:1.1.2
    class: com.hurence.logisland.processor.datastore.BulkPut
    tags: [datastore, record, put, bulk]
  - name: MultiGet
    description:  >
      Retrieves a content from datastore using datastore multiget queries.
      Each incoming record contains information regarding the datastore multiget query that will be performed. This information is stored in record fields whose names are configured in the plugin properties (see below) :
      
       - collection (String) : name of the datastore collection on which the multiget query will be performed. This field is mandatory and should not be empty, otherwise an error output record is sent for this specific incoming record.
       - type (String) : name of the datastore type on which the multiget query will be performed. This field is not mandatory.
       - ids (String) : comma separated list of document ids to fetch. This field is mandatory and should not be empty, otherwise an error output record is sent for this specific incoming record.
       - includes (String) : comma separated list of patterns to filter in (include) fields to retrieve. Supports wildcards. This field is not mandatory.
       - excludes (String) : comma separated list of patterns to filter out (exclude) fields to retrieve. Supports wildcards. This field is not mandatory.
      
      Each outcoming record holds data of one datastore retrieved document. This data is stored in these fields :
      
       - collection (same field name as the incoming record) : name of the datastore collection.
       - type (same field name as the incoming record) : name of the datastore type.
       - id (same field name as the incoming record) : retrieved document id.
       - a list of String fields containing :
      
        - field name : the retrieved field name
        - field value : the retrieved field value
    category: datastore
    module: com.hurence.logisland:logisland-processor-common:1.1.2
    class: com.hurence.logisland.processor.datastore.MultiGet
    tags: [datastore, get, multiget]
  - name: AddFields
    description: Add one or more field to records
    category: processing
    module: com.hurence.logisland:logisland-processor-common:1.1.2
    class: com.hurence.logisland.processor.AddFields
    tags: [record, fields, Add]
  - name: ApplyRegexp
    description: This processor is used to create a new set of fields from one field (using regexp).
    category: processing
    module: com.hurence.logisland:logisland-processor-common:1.1.2
    class: com.hurence.logisland.processor.ApplyRegexp
    tags: [parser, regex, log, record]
  - name: ConvertFieldsType
    description: Converts a field value into the given type. does nothing if conversion is not possible
    category: processing
    module: com.hurence.logisland:logisland-processor-common:1.1.2
    class: com.hurence.logisland.processor.ConvertFieldsType
    tags: [type, fields, update, convert]
  - name: ConvertSimpleDateFormatFields
    description: Convert one or more field representing a date into a Unix Epoch Time (time in milliseconds since &st January 1970, 00:00:00 GMT)...
    category: processing
    module: com.hurence.logisland:logisland-processor-common:1.1.2
    class: com.hurence.logisland.processor.ConvertSimpleDateFormatFields
    tags: [record, fields, Add]
  - name: ExpandMapFields
    description: Expands the content of a MAP field to the root.
    category: processing
    module: com.hurence.logisland:logisland-processor-common:1.1.2
    class: com.hurence.logisland.processor.ExpandMapFields
    tags: [record, fields, Expand, Map]
  - name: FilterRecords
    description: Keep only records based on a given field value
    category: processing
    module: com.hurence.logisland:logisland-processor-common:1.1.2
    class: com.hurence.logisland.processor.FilterRecords
    tags: [record, fields, remove, delete]
  - name: FlatMap
    description: Converts each field records into a single flatten record...
    category: processing
    module: com.hurence.logisland:logisland-processor-common:1.1.2
    class: com.hurence.logisland.processor.FlatMap
    tags: [record, fields, flatmap, flatten]
  - name: GenerateRandomRecord
    description: This is a processor that make random records given an Avro schema
    category: processing
    module: com.hurence.logisland:logisland-processor-common:1.1.2
    class: com.hurence.logisland.processor.GenerateRandomRecord
    tags: [record, avro, generator]
  - name: ModifyId
    description: modify id of records or generate it following defined rules
    category: processing
    module: com.hurence.logisland:logisland-processor-common:1.1.2
    class: com.hurence.logisland.processor.ModifyId
    tags: [record, id, idempotent, generate, modify]
  - name: NormalizeFields
    description: Changes the name of a field according to a provided name mapping...
    category: processing
    module: com.hurence.logisland:logisland-processor-common:1.1.2
    class: com.hurence.logisland.processor.NormalizeFields
    tags: [record, fields, normalizer]
  - name: RemoveFields
    description: Removes a list of fields defined by a comma separated list of field names or keeps only fields defined by a comma separated list of field names.
    category: processing
    module: com.hurence.logisland:logisland-processor-common:1.1.2
    class: com.hurence.logisland.processor.RemoveFields
    tags: [record, fields, remove, delete, keep]
  - name: SelectDistinctRecords
    description: Keep only distinct records based on a given field
    category: processing
    module: com.hurence.logisland:logisland-processor-common:1.1.2
    class: com.hurence.logisland.processor.SelectDistinctRecords
    tags: [record, fields, remove, delete]
  - name: EvaluateJsonPath
    description: Evaluates one or more JsonPath expressions against the content of a FlowFile. The results of those expressions are assigned to Records Fields depending on configuration of the Processor. JsonPaths are entered by adding user-defined properties; the name of the property maps to the Field Name into which the result will be placed. The value of the property must be a valid JsonPath expression. A Return Type of 'auto-detect' will make a determination based off the configured destination. If the JsonPath evaluates to a JSON array or JSON object and the Return Type is set to 'scalar' the Record will be routed to error. A Return Type of JSON can return scalar values if the provided JsonPath evaluates to the specified value. If the expression matches nothing, Fields will be created with empty strings as the value 
    category: parsing
    module: com.hurence.logisland:logisland-processor-common:1.1.2
    class: com.hurence.logisland.processor.EvaluateJsonPath
    tags: [JSON, evaluate, JsonPath]
  - name: ParseProperties
    description:  >
      Parse a field made of key=value fields separated by spaces
      a string like "a=1 b=2 c=3" will add a,b & c fields, respectively with values 1,2 & 3 to the current Record
    category: parsing
    module: com.hurence.logisland:logisland-processor-common:1.1.2
    class: com.hurence.logisland.processor.ParseProperties
    tags: [record, properties, parser]
  - name: SetJsonAsFields
    description: The SetJsonAsFields processor reads the content of a string field containing a json  string and sets each json attribute as a field of the current record. Note that this could be achieved with the EvaluateJsonPath processor, but this implies to declare each json first level attribute in the configuration and also to know by advance every one of them. Whereas for this simple case, the SetJsonAsFields processor does not require such a configuration and will work with any incoming json, regardless of the list of first level attributes.
    category: parsing
    module: com.hurence.logisland:logisland-processor-common:1.1.2
    class: com.hurence.logisland.processor.SetJsonAsFields
    tags: [json]
  - name: SplitField
    description: This processor is used to create a new set of fields from one field (using split).
    category: parsing
    module: com.hurence.logisland:logisland-processor-common:1.1.2
    class: com.hurence.logisland.processor.SplitField
    tags: [parser, split, log, record]
  - name: SplitText
    description: This is a processor that is used to split a String into fields according to a given Record mapping
    category: parsing
    module: com.hurence.logisland:logisland-processor-common:1.1.2
    class: com.hurence.logisland.processor.SplitText
    tags: [parser, regex, log, record]
  - name: SplitTextMultiline
    description: No description provided.
    category: parsing
    module: com.hurence.logisland:logisland-processor-common:1.1.2
    class: com.hurence.logisland.processor.SplitTextMultiline
  - name: SplitTextWithProperties
    description: This is a processor that is used to split a String into fields according to a given Record mapping
    category: parsing
    module: com.hurence.logisland:logisland-processor-common:1.1.2
    class: com.hurence.logisland.processor.SplitTextWithProperties
    tags: [parser, regex, log, record]
  - name: CheckAlerts
    description: Add one or more records representing alerts. Using a datastore.
    category: alerting
    module: com.hurence.logisland:logisland-processor-common:1.1.2
    class: com.hurence.logisland.processor.alerting.CheckAlerts
    tags: [record, alerting, thresholds, opc, tag]
  - name: CheckThresholds
    description:  >
      Compute threshold cross from given formulas.
      
      - each dynamic property will return a new record according to the formula definition
      - the record name will be set to the property name
      - the record time will be set to the current timestamp
    category: alerting
    module: com.hurence.logisland:logisland-processor-common:1.1.2
    class: com.hurence.logisland.processor.alerting.CheckThresholds
    tags: [record, threshold, tag, alerting]
  - name: SendMail
    description:  >
      The SendMail processor is aimed at sending an email (like for instance an alert email) from an incoming record. There are three ways an incoming record can generate an email according to the special fields it must embed. Here is a list of the record fields that generate a mail and how they work:
      
      - **mail_text**: this is the simplest way for generating a mail. If present, this field means to use its content (value) as the payload of the mail to send. The mail is sent in text format if there is only this special field in the record. Otherwise, used with either mail_html or mail_use_template, the content of mail_text is the aletrnative text to the HTML mail that is generated.
      
      - **mail_html**: this field specifies that the mail should be sent as HTML and the value of the field is mail payload. If mail_text is also present, its value is used as the alternative text for the mail. mail_html cannot be used with mail_use_template: only one of those two fields should be present in the record.
      
      - **mail_use_template**: If present, this field specifies that the mail should be sent as HTML and the HTML content is to be generated from the template in the processor configuration key **html.template**. The template can contain parameters which must also be present in the record as fields. See documentation of html.template for further explanations. mail_use_template cannot be used with mail_html: only one of those two fields should be present in the record.
      
       If **allow_overwrite** configuration key is true, any mail.* (dot format) configuration key may be overwritten with a matching field in the record of the form mail_* (underscore format). For instance if allow_overwrite is true and mail.to is set to config_address@domain.com, a record generating a mail with a mail_to field set to record_address@domain.com will send a mail to record_address@domain.com.
      
       Apart from error records (when he is unable to process the incoming record or to send the mail), this processor is not expected to produce any output records.
    category: alerting
    module: com.hurence.logisland:logisland-processor-common:1.1.2
    class: com.hurence.logisland.processor.SendMail
    tags: [smtp, email, e-mail, mail, mailer, sendmail, message, alert, html]
  - name: ParseUserAgent
    description: The user-agent processor allows to decompose User-Agent value from an HTTP header into several attributes of interest. There is no standard format for User-Agent strings, hence it is not easily possible to use regexp to handle them. This processor rely on the `YAUAA library <https://github.com/nielsbasjes/yauaa>`_ to do the heavy work.
    category: enrichment
    module: com.hurence.logisland:logisland-processor-useragent:1.1.2
    class: com.hurence.logisland.processor.useragent.ParseUserAgent
    tags: [User-Agent, clickstream, DMP]
  - name: ConsolidateSession
    description:  >
      The ConsolidateSession processor is the Logisland entry point to get and process events from the Web Analytics.As an example here is an incoming event from the Web Analytics:
      
      "fields": [{ "name": "timestamp",              "type": "long" },{ "name": "remoteHost",             "type": "string"},{ "name": "record_type",            "type": ["null", "string"], "default": null },{ "name": "record_id",              "type": ["null", "string"], "default": null },{ "name": "location",               "type": ["null", "string"], "default": null },{ "name": "hitType",                "type": ["null", "string"], "default": null },{ "name": "eventCategory",          "type": ["null", "string"], "default": null },{ "name": "eventAction",            "type": ["null", "string"], "default": null },{ "name": "eventLabel",             "type": ["null", "string"], "default": null },{ "name": "localPath",              "type": ["null", "string"], "default": null },{ "name": "q",                      "type": ["null", "string"], "default": null },{ "name": "n",                      "type": ["null", "int"],    "default": null },{ "name": "referer",                "type": ["null", "string"], "default": null },{ "name": "viewportPixelWidth",     "type": ["null", "int"],    "default": null },{ "name": "viewportPixelHeight",    "type": ["null", "int"],    "default": null },{ "name": "screenPixelWidth",       "type": ["null", "int"],    "default": null },{ "name": "screenPixelHeight",      "type": ["null", "int"],    "default": null },{ "name": "partyId",                "type": ["null", "string"], "default": null },{ "name": "sessionId",              "type": ["null", "string"], "default": null },{ "name": "pageViewId",             "type": ["null", "string"], "default": null },{ "name": "is_newSession",          "type": ["null", "boolean"],"default": null },{ "name": "userAgentString",        "type": ["null", "string"], "default": null },{ "name": "pageType",               "type": ["null", "string"], "default": null },{ "name": "UserId",                 "type": ["null", "string"], "default": null },{ "name": "B2Bunit",                "type": ["null", "string"], "default": null },{ "name": "pointOfService",         "type": ["null", "string"], "default": null },{ "name": "companyID",              "type": ["null", "string"], "default": null },{ "name": "GroupCode",              "type": ["null", "string"], "default": null },{ "name": "userRoles",              "type": ["null", "string"], "default": null },{ "name": "is_PunchOut",            "type": ["null", "string"], "default": null }]The ConsolidateSession processor groups the records by sessions and compute the duration between now and the last received event. If the distance from the last event is beyond a given threshold (by default 30mn), then the session is considered closed.The ConsolidateSession is building an aggregated session object for each active session.This aggregated object includes: - The actual session duration. - A boolean representing wether the session is considered active or closed.   Note: it is possible to ressurect a session if for instance an event arrives after a session has been marked closed. - User related infos: userId, B2Bunit code, groupCode, userRoles, companyId - First visited page: URL - Last visited page: URL The properties to configure the processor are: - sessionid.field:          Property name containing the session identifier (default: sessionId). - timestamp.field:          Property name containing the timestamp of the event (default: timestamp). - session.timeout:          Timeframe of inactivity (in seconds) after which a session is considered closed (default: 30mn). - visitedpage.field:        Property name containing the page visited by the customer (default: location). - fields.to.return:         List of fields to return in the aggregated object. (default: N/A)
    category: analytics
    module: com.hurence.logisland:logisland-processor-web-analytics:1.1.2
    class: com.hurence.logisland.processor.webAnalytics.ConsolidateSession
    tags: [analytics, web, session]
  - name: DetectOutliers
    description:  >
      Outlier Analysis: A Hybrid Approach
      
      In order to function at scale, a two-phase approach is taken
      
      For every data point
      
      - Detect outlier candidates using a robust estimator of variability (e.g. median absolute deviation) that uses distributional sketching (e.g. Q-trees)
      - Gather a biased sample (biased by recency)
      - Extremely deterministic in space and cheap in computation
      
      For every outlier candidate
      
      - Use traditional, more computationally complex approaches to outlier analysis (e.g. Robust PCA) on the biased sample
      - Expensive computationally, but run infrequently
      
      This becomes a data filter which can be attached to a timeseries data stream within a distributed computational framework (i.e. Storm, Spark, Flink, NiFi) to detect outliers.
    category: analytics
    module: com.hurence.logisland:logisland-processor-outlier-detection:1.1.2
    class: com.hurence.logisland.processor.DetectOutliers
    tags: [analytic, outlier, record, iot, timeseries]
  - name: IncrementalWebSession
    description:  >
      This processor creates and updates web-sessions based on incoming web-events. Note that both web-sessions and web-events are stored in elasticsearch.
       Firstly, web-events are grouped by their session identifier and processed in chronological order.
       Then each web-session associated to each group is retrieved from elasticsearch.
       In case none exists yet then a new web session is created based on the first web event.
       The following fields of the newly created web session are set based on the associated web event: session identifier, first timestamp, first visited page. Secondly, once created, or retrieved, the web session is updated by the remaining web-events.
       Updates have impacts on fields of the web session such as event counter, last visited page,  session duration, ...
       Before updates are actually applied, checks are performed to detect rules that would trigger the creation of a new session:
      
      	the duration between the web session and the web event must not exceed the specified time-out,
      	the web session and the web event must have timestamps within the same day (at midnight a new web session is created),
      	source of traffic (campaign, ...) must be the same on the web session and the web event.
      
       When a breaking rule is detected, a new web session is created with a new session identifier where as remaining web-events still have the original session identifier. The new session identifier is the original session suffixed with the character '#' followed with an incremented counter. This new session identifier is also set on the remaining web-events.
       Finally when all web events were applied, all web events -potentially modified with a new session identifier- are save in elasticsearch. And web sessions are passed to the next processor.
      
      WebSession information are:
      - first and last visited page
      - first and last timestamp of processed event 
      - total number of processed events
      - the userId
      - a boolean denoting if the web-session is still active or not
      - an integer denoting the duration of the web-sessions
      - optional fields that may be retrieved from the processed events
    category: analytics
    module: com.hurence.logisland:logisland-processor-web-analytics:1.1.2
    class: com.hurence.logisland.processor.webAnalytics.IncrementalWebSession
    tags: [analytics, web, session]
  - name: SetSourceOfTraffic
    description:  >
      Compute the source of traffic of a web session. Users arrive at a website or application through a variety of sources, 
      including advertising/paying campaigns, search engines, social networks, referring sites or direct access. 
      When analysing user experience on a webshop, it is crucial to collect, process, and report the campaign and traffic-source data. 
      To compute the source of traffic of a web session, the user has to provide the utm_* related properties if available
      i-e: **utm_source.field**, **utm_medium.field**, **utm_campaign.field**, **utm_content.field**, **utm_term.field**)
      , the referer (**referer.field** property) and the first visited page of the session (**first.visited.page.field** property).
      By default the source of traffic information are placed in a flat structure (specified by the **source_of_traffic.suffix** property
      with a default value of source_of_traffic). To work properly the SetSourceOfTraffic processor needs to have access to an 
      Elasticsearch index containing a list of the most popular search engines and social networks. The ES index (specified by the **es.index** property) should be structured such that the _id of an ES document MUST be the name of the domain. If the domain is a search engine, the related ES doc MUST have a boolean field (default being search_engine) specified by the property **es.search_engine.field** with a value set to true. If the domain is a social network , the related ES doc MUST have a boolean field (default being social_network) specified by the property **es.social_network.field** with a value set to true. 
    category: analytics
    module: com.hurence.logisland:logisland-processor-web-analytics:1.1.2
    class: com.hurence.logisland.processor.webAnalytics.SetSourceOfTraffic
    tags: [session, traffic, source, web, analytics]
  - name: URLDecoder
    description:  >
      Decode one or more field containing an URL with possibly special chars encoded
      ...
    category: analytics
    module: com.hurence.logisland:logisland-processor-web-analytics:1.1.2
    class: com.hurence.logisland.processor.webAnalytics.URLDecoder
    tags: [record, fields, Decode]
  - name: EnrichRecordsElasticsearch
    description:  >
      Enrich input records with content indexed in elasticsearch using multiget queries.
      Each incoming record must be possibly enriched with information stored in elasticsearch. 
      Each outcoming record holds at least the input record plus potentially one or more fields coming from of one elasticsearch document.
    category: enrichment
    module: com.hurence.logisland:logisland-processor-elasticsearch:1.1.2
    class: com.hurence.logisland.processor.elasticsearch.EnrichRecordsElasticsearch
    tags: [elasticsearch]
  - name: IpToFqdn
    description: Translates an IP address into a FQDN (Fully Qualified Domain Name). An input field from the record has the IP as value. An new field is created and its value is the FQDN matching the IP address. The resolution mechanism is based on the underlying operating system. The resolution request may take some time, specially if the IP address cannot be translated into a FQDN. For these reasons this processor relies on the logisland cache service so that once a resolution occurs or not, the result is put into the cache. That way, the real request for the same IP is not re-triggered during a certain period of time, until the cache entry expires. This timeout is configurable but by default a request for the same IP is not triggered before 24 hours to let the time to the underlying DNS system to be potentially updated.
    category: enrichment
    module: com.hurence.logisland:logisland-processor-enrichment:1.1.2
    class: com.hurence.logisland.processor.enrichment.IpToFqdn
    tags: [dns, ip, fqdn, domain, address, fqhn, reverse, resolution, enrich]
  - name: IpToGeo
    description: Looks up geolocation information for an IP address. The attribute that contains the IP address to lookup must be provided in the **ip.address.field** property. By default, the geo information are put in a hierarchical structure. That is, if the name of the IP field is 'X', then the the geo attributes added by enrichment are added under a father field named X_geo. "_geo" is the default hierarchical suffix that may be changed with the **geo.hierarchical.suffix** property. If one wants to put the geo fields at the same level as the IP field, then the **geo.hierarchical** property should be set to false and then the geo attributes are  created at the same level as him with the naming pattern X_geo_<geo_field>. "_geo_" is the default flat suffix but this may be changed with the **geo.flat.suffix** property. The IpToGeo processor requires a reference to an Ip to Geo service. This must be defined in the **iptogeo.service** property. The added geo fields are dependant on the underlying Ip to Geo service. The **geo.fields** property must contain the list of geo fields that should be created if data is available for  the IP to resolve. This property defaults to "*" which means to add every available fields. If one only wants a subset of the fields,  one must define a comma separated list of fields as a value for the **geo.fields** property. The list of the available geo fields is in the description of the **geo.fields** property.
    category: enrichment
    module: com.hurence.logisland:logisland-processor-enrichment:1.1.2
    class: com.hurence.logisland.processor.enrichment.IpToGeo
    tags: [geo, enrich, ip]
  - name: ParseBroEvent
    description:  >
      The ParseBroEvent processor is the Logisland entry point to get and process `Bro <https://www.bro.org>`_ events. The `Bro-Kafka plugin <https://github.com/bro/bro-plugins/tree/master/kafka>`_ should be used and configured in order to have Bro events sent to Kafka. See the `Bro/Logisland tutorial <http://logisland.readthedocs.io/en/latest/tutorials/indexing-bro-events.html>`_ for an example of usage for this processor. The ParseBroEvent processor does some minor pre-processing on incoming Bro events from the Bro-Kafka plugin to adapt them to Logisland.
      
      Basically the events coming from the Bro-Kafka plugin are JSON documents with a first level field indicating the type of the event. The ParseBroEvent processor takes the incoming JSON document, sets the event type in a record_type field and sets the original sub-fields of the JSON event as first level fields in the record. Also any dot in a field name is transformed into an underscore. Thus, for instance, the field id.orig_h becomes id_orig_h. The next processors in the stream can then process the Bro events generated by this ParseBroEvent processor.
      
      As an example here is an incoming event from Bro:
      
      {
      
         "conn": {
      
           "id.resp_p": 9092,
      
           "resp_pkts": 0,
      
           "resp_ip_bytes": 0,
      
           "local_orig": true,
      
           "orig_ip_bytes": 0,
      
           "orig_pkts": 0,
      
           "missed_bytes": 0,
      
           "history": "Cc",
      
           "tunnel_parents": [],
      
           "id.orig_p": 56762,
      
           "local_resp": true,
      
           "uid": "Ct3Ms01I3Yc6pmMZx7",
      
           "conn_state": "OTH",
      
           "id.orig_h": "172.17.0.2",
      
           "proto": "tcp",
      
           "id.resp_h": "172.17.0.3",
      
           "ts": 1487596886.953917
      
         }
      
       }
      
      It gets processed and transformed into the following Logisland record by the ParseBroEvent processor:
      
      "@timestamp": "2017-02-20T13:36:32Z"
      
      "record_id": "6361f80a-c5c9-4a16-9045-4bb51736333d"
      
      "record_time": 1487597792782
      
      "record_type": "conn"
      
      "id_resp_p": 9092
      
      "resp_pkts": 0
      
      "resp_ip_bytes": 0
      
      "local_orig": true
      
      "orig_ip_bytes": 0
      
      "orig_pkts": 0
      
      "missed_bytes": 0
      
      "history": "Cc"
      
      "tunnel_parents": []
      
      "id_orig_p": 56762
      
      "local_resp": true
      
      "uid": "Ct3Ms01I3Yc6pmMZx7"
      
      "conn_state": "OTH"
      
      "id_orig_h": "172.17.0.2"
      
      "proto": "tcp"
      
      "id_resp_h": "172.17.0.3"
      
      "ts": 1487596886.953917
    category: security
    module: com.hurence.logisland:logisland-processor-cyber-security:1.1.2
    class: com.hurence.logisland.processor.bro.ParseBroEvent
    tags: [bro, security, IDS, NIDS]
  - name: ParseNetflowEvent
    description:  >
      The `Netflow V5 <http://www.cisco.com/c/en/us/td/docs/ios/solutions_docs/netflow/nfwhite.html>`_ processor is the Logisland entry point to  process Netflow (V5) events. NetFlow is a feature introduced on Cisco routers that provides the ability to collect IP network traffic.We can distinguish 2 components:
      
      	- Flow exporter: aggregates packets into flows and exports flow records (binary format) towards one or more flow collectors
      
      	- Flow collector: responsible for reception, storage and pre-processing of flow data received from a flow exporter
      
      The collected data are then available for analysis purpose (intrusion detection, traffic analysis...)
      Netflow are sent to kafka in order to be processed by logisland.
      In the tutorial we will simulate Netflow traffic using `nfgen <https://github.com/pazdera/NetFlow-Exporter-Simulator>`_. this traffic will be sent to port 2055. The we rely on nifi to listen of that port for   incoming netflow (V5) traffic and send them to a kafka topic. The Netflow processor could thus treat these events and generate corresponding logisland records. The following processors in the stream can then process the Netflow records generated by this processor.
    category: security
    module: com.hurence.logisland:logisland-processor-cyber-security:1.1.2
    class: com.hurence.logisland.processor.netflow.ParseNetflowEvent
    tags: [netflow, security]
  - name: ParseNetworkPacket
    description: The ParseNetworkPacket processor is the LogIsland entry point to parse network packets captured either off-the-wire (stream mode) or in pcap format (batch mode).  In batch mode, the processor decodes the bytes of the incoming pcap record, where a Global header followed by a sequence of [packet header, packet data] pairs are stored. Then, each incoming pcap event is parsed into n packet records. The fields of packet headers are then extracted and made available in dedicated record fields. See the `Capturing Network packets tutorial <http://logisland.readthedocs.io/en/latest/tutorials/indexing-network-packets.html>`_ for an example of usage of this processor.
    category: security
    module: com.hurence.logisland:logisland-processor-cyber-security:1.1.2
    class: com.hurence.logisland.processor.networkpacket.ParseNetworkPacket
    tags: [PCap, security, IDS, NIDS]
  - name: SampleRecords
    description:  >
      Query matching based on `Luwak <http://www.confluent.io/blog/real-time-full-text-search-with-luwak-and-samza/>`_
      
      you can use this processor to handle custom events defined by lucene queries
      a new record is added to output each time a registered query is matched
      
      A query is expressed as a lucene query against a field like for example: 
      
      .. code::
      
         message:'bad exception'
         error_count:[10 TO *]
         bytes_out:5000
         user_name:tom*
      
      Please read the `Lucene syntax guide <https://lucene.apache.org/core/5_5_0/queryparser/org/apache/lucene/queryparser/classic/package-summary.html#package_description>`_ for supported operations
      
      .. warning::
         don't forget to set numeric fields property to handle correctly numeric ranges queries
    category: timeseries
    module: com.hurence.logisland:logisland-processor-sampling:1.1.2
    class: com.hurence.logisland.processor.SampleRecords
    tags: [analytic, sampler, record, iot, timeseries]
  - name: BulkAddElasticsearch
    description: Indexes the content of a Record in Elasticsearch using elasticsearch's bulk processor
    category: datastore
    module: com.hurence.logisland:logisland-processor-elasticsearch:1.1.2
    class: com.hurence.logisland.processor.elasticsearch.BulkAddElasticsearch
    tags: [elasticsearch]
  - name: FetchHBaseRow
    description: Fetches a row from an HBase table. The Destination property controls whether the cells are added as flow file attributes, or the row is written to the flow file content as JSON. This processor may be used to fetch a fixed row on a interval by specifying the table and row id directly in the processor, or it may be used to dynamically fetch rows by referencing the table and row id from incoming flow files.
    category: datastore
    module: com.hurence.logisland:logisland-processor-hbase:1.1.2
    class: com.hurence.logisland.processor.hbase.FetchHBaseRow
    tags: [hbase, scan, fetch, get, enrich]
  - name: MultiGetElasticsearch
    description:  >
      Retrieves a content indexed in elasticsearch using elasticsearch multiget queries.
      Each incoming record contains information regarding the elasticsearch multiget query that will be performed. This information is stored in record fields whose names are configured in the plugin properties (see below) :
      
       - index (String) : name of the elasticsearch index on which the multiget query will be performed. This field is mandatory and should not be empty, otherwise an error output record is sent for this specific incoming record.
       - type (String) : name of the elasticsearch type on which the multiget query will be performed. This field is not mandatory.
       - ids (String) : comma separated list of document ids to fetch. This field is mandatory and should not be empty, otherwise an error output record is sent for this specific incoming record.
       - includes (String) : comma separated list of patterns to filter in (include) fields to retrieve. Supports wildcards. This field is not mandatory.
       - excludes (String) : comma separated list of patterns to filter out (exclude) fields to retrieve. Supports wildcards. This field is not mandatory.
      
      Each outcoming record holds data of one elasticsearch retrieved document. This data is stored in these fields :
      
       - index (same field name as the incoming record) : name of the elasticsearch index.
       - type (same field name as the incoming record) : name of the elasticsearch type.
       - id (same field name as the incoming record) : retrieved document id.
       - a list of String fields containing :
      
         * field name : the retrieved field name
         * field value : the retrieved field value
    category: datastore
    module: com.hurence.logisland:logisland-processor-elasticsearch:1.1.2
    class: com.hurence.logisland.processor.elasticsearch.MultiGetElasticsearch
    tags: [elasticsearch]
  - name: PutHBaseCell
    description: Adds the Contents of a Record to HBase as the value of a single cell
    category: datastore
    module: com.hurence.logisland:logisland-processor-hbase:1.1.2
    class: com.hurence.logisland.processor.hbase.PutHBaseCell
    tags: [hadoop, hbase]
  - name: RunPython
    description:  >
       !!!! WARNING !!!!
      
      The RunPython processor is currently an experimental feature : it is delivered as is, with the current set of features and is subject to modifications in API or anything else in further logisland releases without warnings. There is no tutorial yet. If you want to play with this processor, use the python-processing.yml example and send the apache logs of the index apache logs tutorial. The debug stream processor at the end of the stream should output events in stderr file of the executors from the spark console.
      
      This processor allows to implement and run a processor written in python. This can be done in 2 ways. Either directly defining the process method code in the **script.code.process** configuration property or poiting to an external python module script file in the **script.path** configuration property. Directly defining methods is called the inline mode whereas using a script file is called the file mode. Both ways are mutually exclusive. Whether using the inline of file mode, your python code may depend on some python dependencies. If the set of python dependencies already delivered with the Logisland framework is not sufficient, you can use the **dependencies.path** configuration property to give their location. Currently only the nltk python library is delivered with Logisland.
    category: processing
    module: com.hurence.logisland:logisland-processor-scripting:1.1.2
    class: com.hurence.logisland.processor.scripting.python.RunPython
    tags: [scripting, python]
  - name: EvaluateXPath
    description: Evaluates one or more XPaths against the content of a record. The results of those XPaths are assigned to new attributes in the records, depending on configuration of the Processor. XPaths are entered by adding user-defined properties; the name of the property maps to the Attribute Name into which the result will be placed. The value of the property must be a valid XPath expression. If the expression matches nothing, no attributes is added. 
    category: parsing
    module: com.hurence.logisland:logisland-processor-xml:1.1.2
    class: com.hurence.logisland.processor.xml.EvaluateXPath
    tags: [XML, evaluate, XPath]
  - name: ExcelExtract
    description: Consumes a Microsoft Excel document and converts each worksheet's line to a structured record. The processor is assuming to receive raw excel file as input record.
    category: parsing
    module: com.hurence.logisland:logisland-processor-excel:1.1.2
    class: com.hurence.logisland.processor.excel.ExcelExtract
    tags: [excel, processor, poi]
  - name: ParseGitlabLog
    description: The Gitlab logs processor is the Logisland entry point to get and process `Gitlab <https://www.gitlab.com>`_ logs. This allows for instance to monitor activities in your Gitlab server. The expected input of this processor are records from the production_json.log log file of Gitlab which contains JSON records. You can for instance use the `kafkacat <https://github.com/edenhill/kafkacat>`_ command to inject those logs into kafka and thus Logisland.
    category: parsing
    module: com.hurence.logisland:logisland-processor-common-logs:1.1.2
    class: com.hurence.logisland.processor.commonlogs.gitlab.ParseGitlabLog
    tags: [logs, gitlab]
  - name: MatchIP
    description:  >
      IP address Query matching (using `Luwak <http://www.confluent.io/blog/real-time-full-text-search-with-luwak-and-samza/>)`_
      
      You can use this processor to handle custom events matching IP address (CIDR)
      The record sent from a matching an IP address record is tagged appropriately.
      
      A query is expressed as a lucene query against a field like for example: 
      
      .. code::
      
      	message:'bad exception'
      	error_count:[10 TO *]
      	bytes_out:5000
      	user_name:tom*
      
      Please read the `Lucene syntax guide <https://lucene.apache.org/core/5_5_0/queryparser/org/apache/lucene/queryparser/classic/package-summary.html#package_description>`_ for supported operations
      
      .. warning::
      
      	don't forget to set numeric fields property to handle correctly numeric ranges queries
    category: alerting
    module: com.hurence.logisland:logisland-processor-querymatcher:1.1.2
    class: com.hurence.logisland.processor.MatchIP
    tags: [analytic, percolator, record, record, query, lucene]
  - name: MatchQuery
    description:  >
      Query matching based on `Luwak <http://www.confluent.io/blog/real-time-full-text-search-with-luwak-and-samza/>`_
      
      you can use this processor to handle custom events defined by lucene queries
      a new record is added to output each time a registered query is matched
      
      A query is expressed as a lucene query against a field like for example: 
      
      .. code::
      
      	message:'bad exception'
      	error_count:[10 TO *]
      	bytes_out:5000
      	user_name:tom*
      
      Please read the `Lucene syntax guide <https://lucene.apache.org/core/5_5_0/queryparser/org/apache/lucene/queryparser/classic/package-summary.html#package_description>`_ for supported operations
      
      .. warning::
      
      	don't forget to set numeric fields property to handle correctly numeric ranges queries
    category: alerting
    module: com.hurence.logisland:logisland-processor-querymatcher:1.1.2
    class: com.hurence.logisland.processor.MatchQuery
    tags: [analytic, percolator, record, record, query, lucene]
  - name: MaxmindIpToGeoService
    description: Implementation of the IP 2 GEO Service using maxmind lite db file
    category: enrichment
    module: com.hurence.logisland:logisland-service-ip-to-geo-maxmind:1.1.2
    class: com.hurence.logisland.service.iptogeo.maxmind.MaxmindIpToGeoService
    tags: [ip, service, geo, maxmind]
  - name: CSVKeyValueCacheService
    description: A cache that store csv lines as records loaded from a file
    category: datastore
    module: com.hurence.logisland:logisland-service-inmemory-cache:1.1.2
    class: com.hurence.logisland.service.cache.CSVKeyValueCacheService
    tags: [csv, service, cache]
  - name: CassandraControllerService
    description: Provides a controller service that for the moment only allows to bulkput records into cassandra.
    category: datastore
    module: com.hurence.logisland:logisland-service-cassandra-client:1.1.2
    class: com.hurence.logisland.service.cassandra.CassandraControllerService
    tags: [cassandra, service]
  - name: Elasticsearch_2_4_0_ClientService
    description: Implementation of ElasticsearchClientService for Elasticsearch 2.4.0.
    category: datastore
    module: com.hurence.logisland:logisland-service-elasticsearch_2_4_0-client:1.1.2
    class: com.hurence.logisland.service.elasticsearch.Elasticsearch_2_4_0_ClientService
    tags: [elasticsearch, client]
  - name: Elasticsearch_5_4_0_ClientService
    description: Implementation of ElasticsearchClientService for Elasticsearch 5.4.0.
    category: datastore
    module: com.hurence.logisland:logisland-service-elasticsearch_5_4_0-client:1.1.2
    class: com.hurence.logisland.service.elasticsearch.Elasticsearch_5_4_0_ClientService
    tags: [elasticsearch, client]
  - name: HBase_1_1_2_ClientService
    description: Implementation of HBaseClientService for HBase 1.1.2. This service can be configured by providing a comma-separated list of configuration files, or by specifying values for the other properties. If configuration files are provided, they will be loaded first, and the values of the additional properties will override the values from the configuration files. In addition, any user defined properties on the processor will also be passed to the HBase configuration.
    category: datastore
    module: com.hurence.logisland:logisland-service-hbase_1_1_2-client:1.1.2
    class: com.hurence.logisland.service.hbase.HBase_1_1_2_ClientService
    tags: [hbase, client]
  - name: InfluxDBControllerService
    description: Provides a controller service that for the moment only allows to bulkput records into influxdb.
    category: datastore
    module: com.hurence.logisland:logisland-service-influxdb-client:1.1.2
    class: com.hurence.logisland.service.influxdb.InfluxDBControllerService
    tags: [influxdb, service, time series]
  - name: LRUKeyValueCacheService
    description: A controller service for caching data by key value pair with LRU (last recently used) strategy. using LinkedHashMap
    category: datastore
    module: com.hurence.logisland:logisland-service-inmemory-cache:1.1.2
    class: com.hurence.logisland.service.cache.LRUKeyValueCacheService
    tags: [cache, service, key, value, pair, LRU]
  - name: MongoDBControllerService
    description: Provides a controller service that wraps most of the functionality of the MongoDB driver.
    category: datastore
    module: com.hurence.logisland:logisland-service-mongodb-client:1.1.2
    class: com.hurence.logisland.service.mongodb.MongoDBControllerService
    tags: [mongo, mongodb, service]
  - name: RedisKeyValueCacheService
    description: A controller service for caching records by key value pair with LRU (last recently used) strategy. using LinkedHashMap
    category: datastore
    module: com.hurence.logisland:logisland-service-redis:1.1.2
    class: com.hurence.logisland.redis.service.RedisKeyValueCacheService
    tags: [cache, service, key, value, pair, redis]
  - name: Solr_5_5_5_ClientService
    description: Implementation of ElasticsearchClientService for Solr 5.5.5.
    category: datastore
    module: com.hurence.logisland:logisland-service-solr_5_5_5-client:1.1.2
    class: com.hurence.logisland.service.solr.Solr_5_5_5_ClientService
    tags: [solr, client]
  - name: Solr_6_4_2_ChronixClientService
    description: Implementation of ChronixClientService for Solr 6 4 2
    category: datastore
    module: com.hurence.logisland:logisland-service-solr_chronix_6.4.2-client:1.1.2
    class: com.hurence.logisland.service.solr.Solr_6_4_2_ChronixClientService
    tags: [solr, client]
  - name: Solr_6_6_2_ClientService
    description: Implementation of ElasticsearchClientService for Solr 5.5.5.
    category: datastore
    module: com.hurence.logisland:logisland-service-solr_6_6_2-client:1.1.2
    class: com.hurence.logisland.service.solr.Solr_6_6_2_ClientService
    tags: [solr, client]
