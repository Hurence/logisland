#########################################################################################################
# Logisland configuration script template
#########################################################################################################

version: 1.1.2
documentation: LogIsland analytics main config file. Put here every engine or component config

#########################################################################################################
# engine
engine:
  component: com.hurence.logisland.engine.spark.KafkaStreamProcessingEngine
  type: engine
  documentation: A logisland stream that match custom queries
  configuration:
    spark.app.name: QueryMatchingDemo
    spark.master: local[1]
    spark.monitoring.driver.port: 7092
    spark.driver.memory: 1g
    spark.driver.cores: 1
    spark.executor.memory: 2g
    spark.executor.instances: 4
    spark.executor.cores: 2
    spark.yarn.queue: default
    spark.yarn.maxAppAttempts: 1
    spark.yarn.am.attemptFailuresValidityInterval: 1h
    spark.yarn.max.executor.failures: 20
    spark.yarn.executor.failuresValidityInterval: 1h
    spark.task.maxFailures: 2
    spark.serializer: org.apache.spark.serializer.KryoSerializer
    spark.streaming.batchDuration: 2000
    spark.streaming.backpressure.enabled: false
    spark.streaming.unpersist: false
    spark.streaming.blockInterval: 500
    spark.streaming.kafka.maxRatePerPartition: 3000
    spark.streaming.timeout: -1
    spark.streaming.kafka.maxRetries: 3
    spark.streaming.ui.retainedBatches: 200
    spark.streaming.receiver.writeAheadLog.enable: false
    spark.ui.port: 4050

  streamConfigurations:

    # structure apache log
    - stream: parsing_stream
      component: com.hurence.logisland.stream.spark.KafkaRecordStreamParallelProcessing
      type: stream
      documentation: a processor that converts raw apache logs into structured log records
      configuration:
        kafka.input.topics: logisland_raw
        kafka.output.topics: logisland_events
        kafka.error.topics: logisland_errors
        kafka.input.topics.serializer: none
        kafka.output.topics.serializer: com.hurence.logisland.serializer.JsonSerializer
        kafka.error.topics.serializer: com.hurence.logisland.serializer.JsonSerializer
        kafka.metadata.broker.list: ${KAFKA_BROKERS}
        kafka.zookeeper.quorum: ${ZK_QUORUM}
        kafka.topic.autoCreate: true
        kafka.topic.default.partitions: 4
        kafka.topic.default.replicationFactor: 1
      processorConfigurations:

        # parse apache logs
        - processor: apache_parser
          component: com.hurence.logisland.processor.SplitText
          type: parser
          documentation: a parser that produce events from an apache log REGEX
          configuration:
            record.type: apache_log
            value.regex: (\S+)\s+(\S+)\s+(\S+)\s+\[([\w:\/]+\s[+\-]\d{4})\]\s+"(\S+)\s+(\S+)\s*(\S*)"\s+(\S+)\s+(\S+)
            value.fields: src_ip,identd,user,record_time,http_method,http_query,http_version,http_status,bytes_out

        - processor: field_types_converter
          component: com.hurence.logisland.processor.ConvertFieldsType
          type: processor
          documentation: convert some field to a given type
          configuration:
            bytes_out: long

        - processor: debugStream
          component: com.hurence.logisland.processor.DebugStream
          type: debug
          documentation: a parser to print some debug information

    # aggregate metrics
    - stream: metrics_by_host
      component: com.hurence.logisland.stream.spark.KafkaRecordStreamSQLAggregator
      type: stream
      documentation: a processor that links
      configuration:
        kafka.input.topics: logisland_events
        kafka.output.topics: logisland_aggregations
        kafka.error.topics: logisland_errors
        kafka.input.topics.serializer: com.hurence.logisland.serializer.JsonSerializer
        kafka.output.topics.serializer: com.hurence.logisland.serializer.JsonSerializer
        kafka.error.topics.serializer: com.hurence.logisland.serializer.JsonSerializer
        kafka.metadata.broker.list: ${KAFKA_BROKERS}
        kafka.zookeeper.quorum: ${ZK_QUORUM}
        kafka.topic.autoCreate: true
        kafka.topic.default.partitions: 2
        kafka.topic.default.replicationFactor: 1
        window.duration: 10
        avro.input.schema: >
          {  "version":1,
             "type": "record",
             "name": "com.hurence.logisland.record.apache_log",
             "fields": [
               { "name": "record_errors",   "type": [ {"type": "array", "items": "string"},"null"] },
               { "name": "record_raw_key", "type": ["string","null"] },
               { "name": "record_raw_value", "type": ["string","null"] },
               { "name": "record_id",   "type": ["string"] },
               { "name": "record_time", "type": ["long"] },
               { "name": "record_type", "type": ["string"] },
               { "name": "src_ip",      "type": ["string","null"] },
               { "name": "http_method", "type": ["string","null"] },
               { "name": "bytes_out",   "type": ["long","null"] },
               { "name": "http_query",  "type": ["string","null"] },
               { "name": "http_version","type": ["string","null"] },
               { "name": "http_status", "type": ["string","null"] },
               { "name": "identd",      "type": ["string","null"] },
               { "name": "user",        "type": ["string","null"] }    ]}
        sql.query: >
          SELECT count(*) AS connections_count, avg(bytes_out) AS avg_bytes_out, src_ip, first(record_time) as record_time
          FROM logisland_events
          GROUP BY src_ip
          ORDER BY connections_count DESC
          LIMIT 20
        max.results.count: 1000
        output.record.type: top_client_metrics




    # match threshold queries
#    - stream: aggregation_threshold_matching_stream
#      component: com.hurence.logisland.stream.spark.KafkaRecordStreamParallelProcessing
#      type: stream
#      documentation: a processor that match query in parrallel
#      configuration:
#        kafka.input.topics: logisland_aggregations
#        kafka.output.topics: logisland_alerts
#        kafka.error.topics: logisland_errors
#        kafka.input.topics.serializer: com.hurence.logisland.serializer.JsonSerializer
#        kafka.output.topics.serializer: com.hurence.logisland.serializer.JsonSerializer
#        kafka.error.topics.serializer: com.hurence.logisland.serializer.JsonSerializer
#        kafka.metadata.broker.list: ${KAFKA_BROKERS}
#        kafka.zookeeper.quorum: ${ZK_QUORUM}
#        kafka.topic.autoCreate: true
#        kafka.topic.default.partitions: 2
#        kafka.topic.default.replicationFactor: 1
#      processorConfigurations:
#        - processor: match_query
#          component: com.hurence.logisland.processor.MatchQuery
#          type: processor
#          documentation: a parser that produce alerts from lucene queries
#          configuration:
#            numeric.fields: bytes_out,connections_count
#            too_much_bandwidth: avg_bytes_out:[25000 TO 5000000]
#            too_many_connections: connections_count:[150 TO 300]
#            output.record.type: threshold_alert


    # indexing stream
#    - stream: indexing_stream
#      component: com.hurence.logisland.stream.spark.KafkaRecordStreamParallelProcessing
#      type: stream
#      documentation: a processor that converts raw apache logs into structured log records
#      configuration:
#        kafka.input.topics: logisland_events,logisland_alerts,logisland_aggregations
#        kafka.output.topics: none
#        kafka.error.topics: logisland_errors
#        kafka.input.topics.serializer: com.hurence.logisland.serializer.JsonSerializer
#        kafka.output.topics.serializer: com.hurence.logisland.serializer.JsonSerializer
#        kafka.error.topics.serializer: com.hurence.logisland.serializer.JsonSerializer
#        kafka.metadata.broker.list: ${KAFKA_BROKERS}
#        kafka.zookeeper.quorum: ${ZK_QUORUM}
#        kafka.topic.autoCreate: true
#        kafka.topic.default.partitions: 4
#        kafka.topic.default.replicationFactor: 1
#      processorConfigurations:
#        - processor: es_publisher
#          component: com.hurence.logisland.processor.datastore.BulkPut
#          type: processor
#          documentation: a processor that indexes processed events in elasticsearch
#          configuration:
#            datastore.client.service: datastore_service
#            default.collection: logisland
#            default.type: event
#            timebased.collection: yesterday
#            collection.field: search_index
#            type.field: record_type


