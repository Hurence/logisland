#########################################################################################################
# Logisland Rubix Web Analytics
# Receive web events from eventhubs, treat them then push events and sessions into opendistro cluster
#########################################################################################################

version: 1.4.0
documentation: Logisland Rubix Webanalytics

engine:
  component: com.hurence.logisland.engine.spark.KafkaStreamProcessingEngine
  type: engine
  documentation: Receive web events from kafka, treat them and push events and sessions on opendsitro/es cluster
  configuration:
    spark.app.name: LogislandWebanalytics
    spark.master: local[3]

  controllerServiceConfigurations:

    # Kafka sink configuration
    - controllerService: kafka_service
      component: com.hurence.logisland.stream.spark.structured.provider.KafkaStructuredStreamProviderServiceReader
      configuration:
#        output.mode: "complete"
        kafka.input.topics: test-simple
        kafka.output.topics: logisland_out
        kafka.error.topics: logisland_errors
        kafka.input.topics.serializer: com.hurence.logisland.serializer.ExtendedJsonSerializer
        kafka.output.topics.serializer: com.hurence.logisland.serializer.ExtendedJsonSerializer
        kafka.error.topics.serializer: com.hurence.logisland.serializer.ExtendedJsonSerializer
#        kafka.metadata.broker.list: kafka:9092
        kafka.zookeeper.quorum: ${kafka.zookeeper.quorum}
        kafka.topic.autoCreate: true
        kafka.topic.default.partitions: 1
        kafka.topic.default.replicationFactor: 1

    - controllerService: console_service
      component: com.hurence.logisland.stream.spark.structured.provider.ConsoleStructuredStreamProviderService
      configuration:
        output.mode: "update"
        truncate: false


#    - controllerService: opendistro_service
#      component: com.hurence.logisland.service.elasticsearch.Elasticsearch_7_x_ClientService
#      type: service
#      documentation: OpenDistro Service
#      configuration:
#        # SVAZ-VLP-OD01: 10.232.68.16
#        # SVAZ-VLP-OD02: 110.232.68.11
#        # SVAZ-VLP-OD03: 10.232.68.17
#        # SVAZ-VLP-OD04: 10.232.68.15
#        # SVAZ-VLP-OD05: 10.232.68.18
#        hosts: 10.232.68.16:9200,10.232.68.11:9200,10.232.68.17:9200,10.232.68.15:9200,10.232.68.18:9200
#        username: admin
#        password: admin
#        batch.size: 2000
#        flush.interval: 2

#    - controllerService: elasticsearch_service
#        component: com.hurence.logisland.service.elasticsearch.Elasticsearch_2_4_0_ClientService
#        type: service
#        documentation: elasticsearch 2.4.0 service implementation
#        configuration:
#          hosts: ns3004505.ovh.net:9300,ns3005860.ovh.net:9300,ns3050034.ovh.net:9300
#          cluster.name: iph-cluster
#          batch.size: 2000
#          flush.interval: 2

#    - controllerService: lru_cache_service_domain
#      component: com.hurence.logisland.service.cache.LRUKeyValueCacheService
#      type: service
#      documentation: Cache service for Ip to Geo service to keep resolved addresses geo information
#      configuration:
#        cache.size: 100000
#
#    - controllerService: ip_to_geo_service
#      component: com.hurence.logisland.service.iptogeo.maxmind.MaxmindIpToGeoService
#      type: service
#      documentation: Service to get geo information from an ip address using maxmind DB
#      configuration:
#        maxmind.database.uri: dbfs:/FileStore/logisland/maxmind/GeoLite2-City.mmdb
#        locale: fr



  streamConfigurations:
    - stream: kafka_to_console
      component: com.hurence.logisland.stream.spark.structured.StructuredStream
      configuration:
        read.stream.service.provider: kafka_service_site_1
        write.stream.service.provider: console_service
      processorConfigurations:
        - processor: debug
          component: com.hurence.logisland.processor.DebugStream
          type: processor
          documentation: print input

    - stream: kafka_to_console
      component: com.hurence.logisland.stream.spark.structured.StructuredStream
      configuration:
        read.stream.service.provider: kafka_service_site_2
        write.stream.service.provider: console_service
      processorConfigurations:
        - processor: debug
          component: com.hurence.logisland.processor.DebugStream
          type: processor
          documentation: print input

    - stream: kafka_to_console
      component: com.hurence.logisland.stream.spark.structured.StructuredStream
      configuration:
        read.stream.service.provider: kafka_service_site_3
        write.stream.service.provider: console_service
      processorConfigurations:
        - processor: debug
          component: com.hurence.logisland.processor.DebugStream
          type: processor
          documentation: print input
