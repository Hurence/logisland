#########################################################################################################
# Logisland configuration script example: parse network packets and display them in Kibana
#########################################################################################################

version: 1.0.0-RC1
documentation: LogIsland analytics main config file. Put here every engine or component config

#########################################################################################################
# engine
engine:
  component: com.hurence.logisland.engine.spark.KafkaStreamProcessingEngine
  type: engine
  documentation: Parse network packets with LogIsland
  configuration:
    spark.app.name: ParseNetworkPacketDemo
    spark.master: local[4]
    spark.driver.memory: 1G
    spark.driver.cores: 1
    spark.executor.memory: 2G
    spark.executor.instances: 4
    spark.executor.cores: 2
    spark.yarn.queue: default
    spark.yarn.maxAppAttempts: 4
    spark.yarn.am.attemptFailuresValidityInterval: 1h
    spark.yarn.max.executor.failures: 20
    spark.yarn.executor.failuresValidityInterval: 1h
    spark.task.maxFailures: 8
    spark.serializer: org.apache.spark.serializer.KryoSerializer
    spark.streaming.batchDuration: 4000
    spark.streaming.backpressure.enabled: false
    spark.streaming.unpersist: false
    spark.streaming.blockInterval: 500
    spark.streaming.kafka.maxRatePerPartition: 3000
    spark.streaming.timeout: -1
    spark.streaming.unpersist: false
    spark.streaming.kafka.maxRetries: 3
    spark.streaming.ui.retainedBatches: 200
    spark.streaming.receiver.writeAheadLog.enable: false
    spark.ui.port: 4050

  controllerServiceConfigurations:

    - controllerService: elasticsearch_service
      component: com.hurence.logisland.service.elasticsearch.Elasticsearch_2_4_0_ClientService
      type: service
      documentation: elasticsearch 2.4.0 service implementation
      configuration:
        hosts: ${ES_HOSTS}
        cluster.name: ${ES_CLUSTER_NAME}
        batch.size: 20000

  streamConfigurations:

    # Parsing
    - stream: parsing_stream
      component: com.hurence.logisland.stream.spark.KafkaRecordStreamParallelProcessing
      type: stream
      documentation: A processor chain that parses network packets into Logisland records
      configuration:
        kafka.input.topics: logisland_input_packets_topic
        kafka.output.topics: logisland_parsed_packets_topic
        kafka.error.topics: logisland_error_packets_topic
        kafka.input.topics.serializer: com.hurence.logisland.serializer.BytesArraySerializer
        kafka.output.topics.serializer: com.hurence.logisland.serializer.KryoSerializer 
        kafka.error.topics.serializer: com.hurence.logisland.serializer.JsonSerializer
        kafka.metadata.broker.list: ${KAFKA_BROKERS}
        kafka.zookeeper.quorum: ${ZK_QUORUM}
        kafka.topic.autoCreate: true
        kafka.topic.default.partitions: 2
        kafka.topic.default.replicationFactor: 1
      processorConfigurations:

        # Transform network packets into LogIsland packet records
        - processor: ParseNetworkPacket processor
          component: com.hurence.logisland.processor.networkpacket.ParseNetworkPacket
          type: parser
          documentation: A processor that parses network packets into LogIsland records
          configuration:
            debug: true
            flow.mode: stream


    # Indexing
    - stream: indexing_stream
      component: com.hurence.logisland.stream.spark.KafkaRecordStreamParallelProcessing
      type: processor
      documentation: a processor that push events to ES
      configuration:
        kafka.input.topics: logisland_parsed_packets_topic
        kafka.output.topics: none
        kafka.error.topics: logisland_error_packets_topic
        kafka.input.topics.serializer: com.hurence.logisland.serializer.KryoSerializer
        kafka.output.topics.serializer: none
        kafka.error.topics.serializer: com.hurence.logisland.serializer.JsonSerializer
        kafka.metadata.broker.list: ${KAFKA_BROKERS}
        kafka.zookeeper.quorum: ${ZK_QUORUM}
        kafka.topic.autoCreate: true
        kafka.topic.default.partitions: 2
        kafka.topic.default.replicationFactor: 1
      processorConfigurations:

        # Bulk add to elasticsearch
        - processor: ES Publisher
          component: com.hurence.logisland.processor.elasticsearch.BulkAddElasticsearch
          type: processor
          documentation: A processor that pushes network packet records into ES
          configuration:
            elasticsearch.client.service: elasticsearch_service
            default.index: packets_index
            default.type: events
            timebased.index: today
            es.index.field: search_index
            es.type.field: record_type

